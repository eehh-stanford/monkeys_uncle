[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Monkey’s Uncle",
    "section": "",
    "text": "Meng’s Big Data Paradox and the Recent Twitter Poll\n\n\n\n\n\n\nstatistics\n\n\nAmerican politics\n\n\nbig data\n\n\n\n\n\n\n\n\n\nAug 24, 2024\n\n\nJames Holland Jones\n\n\n\n\n\n\n\n\n\n\n\n\nTermination Shock\n\n\n\n\n\n\nCliFi\n\n\nclimate change\n\n\nAdaptation\n\n\n\n\n\n\n\n\n\nMay 7, 2024\n\n\nJames Holland Jones\n\n\n\n\n\n\n\n\n\n\n\n\nAABA 2024\n\n\n\n\n\n\nanthropology\n\n\nanti-racism\n\n\nTESCREAL\n\n\n\n\n\n\n\n\n\nApr 1, 2024\n\n\nJames Holland Jones\n\n\n\n\n\n\n\n\n\n\n\n\nThinking About Uncertainty\n\n\n\n\n\n\nuncertainty\n\n\nadaptation\n\n\nESS185\n\n\n\n\n\n\n\n\n\nJan 20, 2024\n\n\nJames Holland Jones\n\n\n\n\n\n\n\n\n\n\n\n\nNovels (Mostly CliFi) from 2023, Cozy and Otherwise\n\n\n\n\n\n\nCliFi\n\n\nnarrative\n\n\n\n\n\n\n\n\n\nJan 9, 2024\n\n\nJames Holland Jones\n\n\n\n\n\n\n\n\n\n\n\n\nImagining Adaptive Societies\n\n\n\n\n\n\nCliFi\n\n\nclimate change\n\n\n\n\n\n\n\n\n\nJan 1, 2024\n\n\nJames Holland Jones\n\n\n\n\n\n\n\n\n\n\n\n\nPlotting Notes for Adaptation Class\n\n\n\n\n\n\nR\n\n\nadaptation\n\n\nESS185\n\n\n\n\n\n\n\n\n\nDec 31, 2023\n\n\nJames Holland Jones\n\n\n\n\n\n\n\n\n\n\n\n\nMonkey’s Uncle Reprise\n\n\n\n\n\n\nnews\n\n\nscicomm\n\n\n\n\n\n\n\n\n\nDec 30, 2023\n\n\nJames Holland Jones\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Monkey’s Uncle Reprise",
    "section": "",
    "text": "I started writing my blog, Monkey’s Uncle, back in 2008. My wife had major surgery and I was a bit distracted from the task of academic writing. I thought that having a blog might allow me to keep writing in a mode that would require less concentration. Somewhat to my surprise, it worked pretty well.\nAs I wrote more, a funny thing happened. People actually read my posts and sometimes this led to collaborations. One of my most productive collaborations came about because Marcel Salathé chanced upon my blog. We went on to write what would be several of the most-cited papers (for example: here here and here) either of us would write (so far, at least).\nIn the last few years, I’ve essentially stopped blogging. Stanford and WordPress never completely got along and it was always a bit of a hassle to maintain. The folder that held the images for blog posts somehow disappeared and now the posts that were supposed to have embedded images just have blind links in them. For a while, Twitter effectively took the place of the blog. Putting together threads describing my work was fun, but with Twitter’s effective demise as a reasonable tool for scientific communication, I’ve found myself wanting to get back into actual blogging. The seed for re-starting my blog was actually planted by John Quiggin’s lament (in a comment on my extra-long Twitter thread) that people don’t blog anymore. I have also been inspired by my interactions with Henry Farrell, who both blogs on his personal site and is a regular contributor to Crooked Timber.\nQuarto has opened up new opportunities for blogging. I already use it for my course notes and my lab manual, so it seemed natural to give it a try for blogging as well. Quarto has the tremendous benefit of supporting technical writing, embedding code, rendering equations, etc. This sounds promising: my previous blog posts had less of this material than I wanted because of technical limitations of the site.\nThere will be a learning curve here, no doubt. The aesthetics are a bit spare so far. Hopefully, I can figure out how to make a nice header, maybe even with my favorite monkey picture.\n\n\n\nRed colobus mother with infant, sitting on the ground, observing the drama taking place in the trees above her. Picture taken in Kibale National Park, Uganda in 2012.\n\n\nMy goal is to keep my posts a bit shorter than during peak Monkey’s Uncle. I mean, seriously, who really wants to read a 7000-word, slightly emo essay on the sad state of contemporary anthropology?\nI don’t use Twitter anymore, but you can find me on Blueksy. I also have a Mastodon account. So far, at least, these are a far cry from peak Academic Twitter, but they have promise. Fewer right-wing trolls and Russian bots, so they’ve got that going for them."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "I am a professor of Environmental Social Sciences in the Stanford Doerr School of Sustainability. My research focuses on human ecology and adaptability, demography and life history theory, and infectious-disease ecology. You can find more information on my lab page."
  },
  {
    "objectID": "about.html#about-this-blog",
    "href": "about.html#about-this-blog",
    "title": "About",
    "section": "",
    "text": "I am a professor of Environmental Social Sciences in the Stanford Doerr School of Sustainability. My research focuses on human ecology and adaptability, demography and life history theory, and infectious-disease ecology. You can find more information on my lab page."
  },
  {
    "objectID": "posts/plot_notes/index.html",
    "href": "posts/plot_notes/index.html",
    "title": "Plotting Notes for Adaptation Class",
    "section": "",
    "text": "I will be teaching my class, Earth System Science 185: Adaptation, again this winter quarter. One of the stated learning outcomes for this class is to be able to “interpret complex scientific figures.” I show the class lots of figures and ask them in all of the problem sets and both exams to interpret scientific figures. These figures are usually theoretical, though they sometimes involve data. This is largely a theory class though, so lots of theoretical ecology, evolution, and complex systems.\nTo supplement lecture this year, I have assembled some notes on interpreting and rendering scientific figures. These notes are highly incomplete, but they represent a pretty good start. I have learned some hard-won tricks for making attractive and clear scientific figures in R and it seemed like a good idea to assemble them in one place. You can find these notes (which I hope to update as the quarter proceeds) here.\nAn example of the sort of scientific figure we work with is the Noy-Meir (1975) model for stability of grazing systems. The following figure represents the core idea of Noy-Meir’s model.\n\n# Logistic Recruitment\nlogistic.recruit &lt;- expression(r*N*(1 - (N/K)^theta))\nno &lt;- 1\nr &lt;- 0.45\nK &lt;- 100\ntheta &lt;- 1\nN &lt;- seq(0,K,length=500)\n\n# Holling Type II Functional Response\nh2 &lt;- expression(a*N/(b + a*x))\nx &lt;- N+1\na &lt;- 0.7\nb &lt;- 3\nplot(N,eval(logistic.recruit), type=\"l\", yaxs=\"i\", lwd=3, axes=FALSE, xlab=\"Relative Producer Density\", ylab=\"Relative Productivity\", ylim=c(0,15))\nbox()\nlines(x, 6*eval(h2), lwd=2)\nlines(x, 12*eval(h2), lwd=2)\n\npoints(c(8.702413, 36.521362, 57.497146, 85.464859), c(3.485678, 10.441517, 11.003335,  5.652690), col=c(\"red\",\"red\",\"green\",\"green\"), cex=2, pch=16)\narrows(36.521362-7.5, 10.441517+2, 36.521362-1, 10.441517+2, code=1, lwd=3, length=0.1, col=grey(0.75))\narrows(36.521362+1, 10.441517+2, 36.521362+7.5, 10.441517+2, code=2, lwd=3, length=0.1, col=grey(0.75))\n#\narrows(57.497146-7.5, 10.441517+2, 57.497146-1, 10.441517+2, code=2, lwd=3, length=0.1, col=grey(0.75))\narrows(57.497146+1, 10.441517+2, 57.497146+7.5, 10.441517+2, code=1, lwd=3, length=0.1, col=grey(0.75))\ntext(c(59.13355, 86.65497), c(11.458140,  6.107494), c(\"F2\", \"F1\"))\n\n\n\n\n\n\n\n\nThe primary producers have a logistic recruitment curve, while grazers follow a Holling Type II functional response. Points where these curves intersect represent equilibria of the system. The green points are stable, whereas the red points are unstable (stability indicated by the grey arrows above the curves). The two consumption curves represent potential slow-forcing of the system from population growth of the consumer. Population growth can easily lead to the loss of the stable equilibria of the system, leading to catastrophic collapse.\nTo be honest, I’m not sure if these notes will ever move beyond class notes. So far, I have four chapters:\n\ninterpreting scientific figures\ndrawing a rogues’ gallery of classic figures from theoretical ecology, behavioral ecology, economics, and complexity theory\n“actual graphs” (i.e., networks)\nnumerically integrating (and plotting!) ODEs\n\nThese are my various non-standard uses for R graphics (i.e., as opposed to actually plotting data!)."
  },
  {
    "objectID": "posts/clifi2023/index.html",
    "href": "posts/clifi2023/index.html",
    "title": "Imagining Adaptive Societies",
    "section": "",
    "text": "This winter, I once again have the great fortune of co-teaching with Margaret Levi and Paula Moya our class entitled Imagining Adaptive Societies. In this class, we explore how speculative fiction can be used as a tool for imagining adaptable, sustainable, and just societies that can respond to the major challenges of our age. When it comes to the topic of environmental change, it’s so easy to get bogged down in dystopia. We make a concerted effort to think about better futures. Frankly, this can be a bit of a challenge, but that’s part of what makes the project interesting.\nMy fascination (some might suggest obsession) with climate fiction, or CliFi, first started when my friend Michael McElliott (a.k.a., Mikl Em), who was the producer of the long-running lecture series at the Long Now Foundation’s hilariously-hipster bar in San Francisco, The Interval, invited me to the Interval’s green room in May of 2017 to hang out with the author and CliFi pioneer Kim Stanley Robinson, who was doing a reading from his recent CliFi novel, New York 2140.\n\n\n\nKim Stanley Robinson, reading from his novel New York 2140, at The Interval, May 9 2017.\n\n\nWith KSR, his entourage, and Mikl, we talked extensively about the role of the world-building one has to do for speculative fiction in helping us think about adaptation, sustainability, and human-environment interactions more generally. Mikl, who was a massive science-fiction nerd, turned to me at one point and said, “you should give a talk on CliFi at the Interval!” I demurred. While it is true that I have read a lot of science fiction and thought a bit about its utility in stimulating imagination, I know nothing about literature or how one even goes about teaching a class rooted in the humanities.\nBut the seed was definitely planted.\nWhen my wife was a fellow at the Center for Advanced Study in the Behavioral Sciences in 2017-2018, I spent a lot of time loitering up on the hill and talking with then-director Margaret Levi and various other smarties like Shaz Attari and Judy Wajcman about CliFi. On a Stanford Alumni Association trip in the summer of 2018, I was inspired by the spectacular locale of the Mashpi Lodge in Ecuador to roll out a first draft of a talk on CliFi. Eventually, I did give a version of the talk at the Interval. I focused on how long-form narrative might fit into the science of behavior change. This led to a nice write-up by the Stanford News Service, where I got to chat about the utility of literature with Pulitzer-Prize-winning author Adam Johnson. That was cool.\nShortly before the COVID-19 pandemic, Margaret Levi and I received a grant from The Changing Human Experience Initiative at Stanford for a project focused on the social imagination in speculative fiction, especially CliFi. There is lots of humanistic scholarship on speculative fiction and there are plenty of physical scientists who weigh in on the topic (and are frequently authors!). But, as noted by Robert Heinlein in his famous 1947 essay, speculative fiction is not fiction about the science and technology, per se, but rather human reactions to new situations caused by science and technology. It seemed to us like there was a real deficit on the social-science scholarship related to world-building and speculative fiction.\nWe had grand plans. Alas, the pandemic really messed them up. It’s a long and not very interesting story—as with so many pandemic-disruption stories—but we decided that the best way to fulfill the spirit of the project was to teach a class. We recruited Paula, who has a long-standing interest in speculative fiction and actually knows about literature!\nSo, here we are. This is the second time we’re teaching the class and it might, alas, be the last. Margaret is retired and winter quarter is her last teaching quarter at Stanford. It should be fun. I’ve learned so much from Margaret and Paula, not to mention our students and TAs. This year, we will read three novels\n\nParable of the Sower by Octavia Butler\nNew York 2140 by Kim Stanley Robinson\nThe Water Knife by Paolo Bacigalupi\n\nIn addition, we will read a number of shorter works, including selections from KSR’s Ministry of the Future, N.K. Jemisin’s novelette, Emergency Skin, and short stories by Malka Older (“Narrative Disorder”) and Charlie Jane Anders (“Because Change was the Ocean and We Lived by her Mercy”). We might even have some visitors, but that’s a secret (for now).\nWe try to triple-team every lecture. I generally talk about the climate science and human ecology; Margaret talks about the governance, history, and social elements of the works; Paula talks about the novels as works of literature, analyzing the plots and characters and generally improving everyone’s analytical game.\nWe start in a week. Here’s hoping my recently-operated-upon ankle has healed enough by then that I can actually walk to class!"
  },
  {
    "objectID": "posts/novels2023/index.html",
    "href": "posts/novels2023/index.html",
    "title": "Novels (Mostly CliFi) from 2023, Cozy and Otherwise",
    "section": "",
    "text": "I managed to read 28 novels in 2023. Most of these were climate fiction since that remains a teaching (and possibly research) interest.\nIn no particular order, my favorite novels from this past year include:\n\nLauren Wilkinson, American Spy\nThomas D. Lee, Perilous Times\nMalka Older, The Mimicking of Known Successes\nRay Nayler, The Mountain in the Sea\n\nAnother book that I really enjoyed, but wasn’t quite self-contained enough for me to list here is A Psalm for the Wild-Built, Becky Chambers’s first novel in her new cozy CliFi series. I’m anxiously awaiting the next volumes in the series.\nBecky Chambers and Annalee Newitz talk about cozy science fiction more generally in their terrific talk at The Interval, entitled Resisting Dystopia. The idea of resisting dystopia is, in fact, one of the guiding principles of the class I am teaching with Margaret Levi and Paula Moya this quarter. It’s not always easy, I’m afraid.\nMalka Older’s amazing The Mimicking of Known Successes is, apparently, also considered to be of the cozy genre, though I found the setting for it pretty terrifying. A devastated Earth and a scholarly outpost on a gas giant inhospitable to life doesn’t strike me as canonically “cozy,” but I guess there is lots of tea-drinking in academic chambers.\nOne of these things is perhaps not like the others, so merits a moment’s reflection. Wilkinson’s American Spy is a masterful modern take on the spy novel. I think that there are three necessary elements to a successful spy novel. First, and most importantly, there must be moral ambiguity. If it’s not morally ambiguous, are you even spying, bro? Second, there needs to be geographic specificity. The reader needs enough detail to be transported to where the action is, whether it is in some exotic locale, the streets of New York City or London, or the back roads of northern Virginia. Finally, what would a spy novel be without tradecraft? And I don’t mean the Q-esque gadgets here. Neither George Smilely nor Marie Mitchell have—nor need—flame-throwing watches or tricked-out sport cars. Wilkinson delivers spectacularly on all fronts. It is right up there with my all-time favorite spy novels—and I’m an often-disappointed sucker for the genre. A fictionalized account of the assassination of the charismatic leftist president of Burkina Faso, Thomas Sankara, American Spy—as with all the best geopolitical fiction— may force you to learn something if you’re not careful (I did)."
  },
  {
    "objectID": "posts/tuncertain/index.html",
    "href": "posts/tuncertain/index.html",
    "title": "Thinking About Uncertainty",
    "section": "",
    "text": "I have been pretty obsessed with risk for a while now. My interest in formal risk management was piqued when I began thinking about life-history strategies for variable environments. The formal analysis of risk seemed like a pretty big absence from life history theory, particularly in the literature pertaining to the evolution of human life histories. I first articulated some of these ideas in a review I wrote for Current Biology back in 2011. A more recent articulation of this work as it relates to life histories is this paper with Mike Price, where we show that fitness maximizers employ nonlinear probability weighting to decisions (with life-history consequences) under risk. This may sound a bit obscure, but it’s important because nonlinear probability weighting is one of the hallmarks of economic irrationality as seen by the Biases and Heuristics school of behavioral economics. Nonlinear probability weighting may not be rational from the orthodox axiomatic perspective, but it will keep you from going extinct! Better alive and “irrational” than extinct and consistent with some dude’s (arbitrary) axioms.\nWithout getting too far into the weeds, the standard approach to decision-making under risk involves choosing the “prospect” (i.e., a course of action with a variable payoff) with the greatest expected utility. That is, for all the prospects you’re choosing between, sum up the products of the utility of each possible outcome and its associated probability (that’s what an expectation is) and pick the one that’s biggest. If we’re thinking about evolution, we would substitute the word “strategy” for “prospect” and, of course, “fitness” for “utility,” but the decision criterion remains the same. We now just imagine that it’s natural selection that is “choosing” in the sense that individuals who employ strategies with the highest expected fitness will come to dominate the population.\nAs Mike’s an my paper suggests, it might actually be more complicated than that."
  },
  {
    "objectID": "posts/tuncertain/index.html#uncertainty-creeps-in",
    "href": "posts/tuncertain/index.html#uncertainty-creeps-in",
    "title": "Thinking About Uncertainty",
    "section": "Uncertainty Creeps In",
    "text": "Uncertainty Creeps In\nMore recently, I’ve done a bit of a re-think on things and have come to the conclusion that the real action for understanding human evolution—and the future of human adaptation—is uncertainty. Colloquially, risk and uncertainty sound similar. They both involve non-deterministic payoffs. But there is a fundamental difference between them that was recognized over a hundred years ago in the publications of two of history’s great economists. Both John Maynard Keynes and Frank Wright published major works in 1921. In Risk, Uncertainty, and Profit, Knight neatly summarized the fundamental distinction between risk and uncertainty, “Uncertainty must be taken in a sense radically distinct from the familiar notion of Risk, from which it has never been properly separated…The essential fact is that ‘risk’ means in some cases a quantity susceptible of measurement, while at other times it is something distinctly not of this character; and there are far-reaching and crucial differences in the bearings of the phenomena depending on which of the two is really present and operating…. It will appear that a measurable uncertainty, or ‘risk’ proper, as we shall use the term, is so far different from an unmeasurable one that it is not in effect an uncertainty at all.”\nA full articulation of his views of uncertainty would not come from Keynes until the later publication of his better-known, General Theory in 1936, but in his 1921 Treatise on Probability, he did express great skepticism about the usefulness of expectations in rational decision-making. The language he used reflected his unorthodox views on probabilities. In part IV of his Treatise, Keynes wrote “The second difficulty, to which attention is called above, is the neglect of the ‘weights’ of arguments in the conception of ‘mathematical expectation.’ … In the present connection the question comes to this—if two probabilities are equal in degree, ought we, in choosing our course of action, to prefer that one which is based on a greater body of knowledge?” Here, he was really saying essentially the same thing as Knight. If we can’t actually measure the probabilities properly, we can’t use expectations to guide our decisions.\nI now have a lot to say about uncertainty and how organisms faced with it might nonetheless make adaptive decisions. I will save this for future posts—and maybe even some peer-reviewed publications? (hope springs eternal). However, for the rest of this post, I will lay out what I think is a pretty heuristic way of thinking about what uncertainty is."
  },
  {
    "objectID": "posts/tuncertain/index.html#uncertainty-is-horizontal",
    "href": "posts/tuncertain/index.html#uncertainty-is-horizontal",
    "title": "Thinking About Uncertainty",
    "section": "Uncertainty is Horizontal",
    "text": "Uncertainty is Horizontal\nSuppose we have absolute uncertainty over the value of some outcome. That is, we have absolutely no idea what the value could be. What we get is a uniform distribution, which looks like a horizontal line drawn at a height such that the area under it sums to one. If we really have no idea whatsoever what the value can be, then this line goes from minus infinity to infinity and is infinitesimally high.\n\nx &lt;- seq(-10,10,length=1000)\nplot(x,dunif(x,-10,10), type=\"n\", axes=FALSE, frame=TRUE,\n     xlab=\"Outcome\", ylab=\"Probability of Outcome\")\narrows(-10,0.05,10,0.05,code=3, lwd=2)\n\n\n\n\n\n\n\n\nWe gather data to help us estimate the outcome. The more informative the data are that, the more precise our estimate will be. The effect that this has on our probability distribution is as though we were simultaneously pushing up at the most likely value compatible with the data and down on the extremes. Note that failure to observe extreme values is itself information and the more trials you go without observing extreme events, the more your beliefs shift to believe that extremes are rare.\n\nplot(x,dunif(x,-10,10), type=\"n\", axes=FALSE, frame=TRUE,\n     xlab=\"Outcome\", ylab=\"Probability of Outcome\")\narrows(-10,0.05,10,0.05,code=3, lwd=2)\narrows(-7,0.051,-7,0.1, col=\"red\", code=1, lwd=2)\narrows(7,0.051,7,0.1, col=\"red\", code=1, lwd=2)\narrows(0,0,0,0.049, col=\"red\", code=2, lwd=2)\n\n\n\n\n\n\n\n##\nplot(x,dnorm(x,0,3.5), type=\"l\", lwd=2, axes=FALSE, frame=TRUE,\n     yaxs=\"i\", ylim=c(0,0.12),\n     xlab=\"Outcome\", ylab=\"Probability of Outcome\")\narrows(-7,0.051,-7,0.1, col=\"red\", code=1, lwd=2)\narrows(7,0.051,7,0.1, col=\"red\", code=1, lwd=2)\narrows(0,0,0,0.049, col=\"red\", code=2, lwd=2)\n\n\n\n\n\n\n\n\nAs we keep adding informative observations, our distribution becomes taller and has shorter tails.\n\nplot(x,dnorm(x), type=\"l\", lwd=2, axes=FALSE, frame=TRUE, \n     yaxs=\"i\", ylim=c(0,0.42), xlim=c(-10,10),\n     xlab=\"Outcome\", ylab=\"Probability of Outcome\")\narrows(-7,0.051,-7,0.1, col=\"red\", code=1, lwd=2)\narrows(7,0.051,7,0.1, col=\"red\", code=1, lwd=2)\narrows(0,0,0,0.049, col=\"red\", code=2, lwd=2)\n\n\n\n\n\n\n\n\nIn the limit, if our data were perfectly informative, we would get a point mass on the “true” value of the outcome. This is a magical world that I frankly don’t believe in—diversity is the rule in biology—but it is useful to think of the two limiting cases that bracket our ability to estimate things: complete uncertainty is a horizontal line; complete certainty is a vertical line.\nSo as we make ourselves less uncertain, we make our probability distribution more vertical around a central point. Returning to the Keynesian perspective on uncertainty, suppose we have an estimate of our outcome, but that we are not at all certain that our probabilities are correct. They have low ‘weights’ in the Keynesian sense. This case is actually nicely formalized in a Bayesian framework when we marginalize, i.e., average our estimates over unknown (nuisance) parameters. The canonical case is estimating a normal mean with unknown variance using the conjugate model. I will save the details for another post, but the punchline is that the distribution of a normal mean with unknown variance under the standard conjugate model is a t-distribution with degrees of freedom equal to the sample size of your data.\nThis may seem unremarkable. We’ve all done t-tests. The thing is, the t-distribution has some pretty wild properties. Most notable among these is that it is heavy-tailed (or fat-tailed). There is a nice, relatively new text (including full preprint) on heavy-tailed distributions here. Heavy-tailed distributions are one way of thinking about uncertainty.\nAgain, I will leave the details for a later post, but the upshot is that heavy-tailed distributions break expectations. In a short paper in 2001, John Geweke pointed out the degeneracy of expected utility under heavy-tailed distributions. I don’t think enough people know about this result and I will write much more about it later.\nFor now, let’s just let is suffice to say that a heavy tail is kind of like adding back that horizontal element to a probability distribution. The tails aren’t really horizontal, but they’re certainly more horizontal than thin-tailed distribution: by definition, the probability of a heavy-tailed variate decays sub-linearly on a log scale (i.e., slower than exponential).\nHere’s a comparison of our normal distribution with a low-df t-distribution. While the two distributions are centered on the same point, the t-distribution is less peaked, has wider shoulders, and much heavier tails. We are less certain of our estimate.\n\nplot(x,dnorm(x), type=\"l\", lwd=2, axes=FALSE, frame=TRUE, \n     yaxs=\"i\", ylim=c(0,0.42), xlim=c(-10,10),\n     xlab=\"Outcome\", ylab=\"Probability of Outcome\")\nlines(x,dt(x,df=1), lwd=2, col=\"red\")"
  },
  {
    "objectID": "posts/tuncertain/index.html#heavy-tails-make-the-impossible-only-unlikely",
    "href": "posts/tuncertain/index.html#heavy-tails-make-the-impossible-only-unlikely",
    "title": "Thinking About Uncertainty",
    "section": "Heavy Tails Make The Impossible Only Unlikely",
    "text": "Heavy Tails Make The Impossible Only Unlikely\nThe existence of a heavy-tailed distribution means that very large (or very small) values are not impossible. We can compare the tails of a normal distribution and a low-df t-distribution.\n\nplot(x,dnorm(x), type=\"l\", lwd=2, axes=FALSE, frame=TRUE, \n     yaxs=\"i\", xaxs=\"i\", ylim=c(0,0.4), xlim=c(0,10),\n     xlab=\"Outcome\", ylab=\"Probability of Outcome\")\nlines(x,dt(x,df=1), lwd=2, col=\"red\")\nabline(h=0.01,lty=3)\naxis(1)\naxis(2)\n\n\n\n\n\n\n\n\nIt’s common to refer to extreme events by the number of standard deviations they are from the mean. As the conventional variable name for standard deviation is \\(\\sigma\\) (i.e., the Greek letter sigma), we call something the magnitude of which is two standard deviations away from the mean a \\(2\\sigma\\) event, and so on. From the figure, we can see that while the probability of observing a \\(4\\sigma\\) event is essentially zero for the normal distribution, the probability of a \\(6\\sigma\\) event is approximately 1% (1 year out of 100) for the \\(t\\) distribution. The probability of a \\(9\\sigma\\) event is 0.5% and of a \\(10\\sigma\\) event is 0.3%. Another way to think about that is that we should expect a \\(10\\sigma\\) event approximately every 300 years or about 12 human generations.\nThe nonzero probability of these very extreme events causes the variance to blow up. The variance is, in fact, infinite for this distribution (and for \\(df=1\\), the mean is undefined since a standard t-distribution with one degree of freedom is also a Cauchy (0,1) distribution).\nIt’s tough to minimize variance when the variance is infinite! It’s also impossible to rank prospects according to their moments (e.g., mean and variance) when the moments aren’t defined.\nOrganisms (like humans) who face real—or Knightian—uncertainty still need to make decisions. If they can’t choose on the basis of expectations, what do they do? It’s an important question and will be the subject of future posts and (hopefully) forthcoming papers."
  },
  {
    "objectID": "posts/aaba2024/index.html",
    "href": "posts/aaba2024/index.html",
    "title": "AABA 2024",
    "section": "",
    "text": "I am recently back from the annual meeting of the American Association of Biological Anthropologists (AABA). Back in my first blogging period, I always liked to do meeting recaps, and this seems like a good tradition for Monkey’s Uncle 2.0.\nA few years back, I made an executive decision that I would just attend AABA on an annual basis. It can be very, very difficult to find a conference where you feel totally comfortable when your research is highly interdisciplinary. I’ve tried many—American Anthropological Association, Population Association of America, Sunbelt Social Networks, Ecology and Evolution of Infectious Disease, Evolution and Human Behavior, Evolutionary Demography, Trop Med—and none really fits quite right. AABA really doesn’t fit especially well either, but it has a number of virtues that distinguish it from every other conference. First, I know lots of people there. While I don’t have a specific posse to hang with, I have lots of friends and colleagues I can count on seeing and, let’s face it, catching up with people is the primary purpose of attending conferences. Second, I really love AABA as an institution. Attending the annual meetings is a bargain, especially compared to the total waste of time and money that is AAA. More importantly, AABA has prioritized making the society diverse and welcoming for a broad community of people, as a society that is dedicated to the scientific study of human diversity should. I find the vibe at AABA just better than most other conferences.\nLA was a nice location—I ate at several lovely food trucks as well as at the Grand Central Market—for the meeting, though I must admit that the venue seemed a bit fancy for AABA standards."
  },
  {
    "objectID": "posts/aaba2024/index.html#undermining-race-science",
    "href": "posts/aaba2024/index.html#undermining-race-science",
    "title": "AABA 2024",
    "section": "Undermining Race Science",
    "text": "Undermining Race Science\nThis year, Charles Roseman and I organized an invited symposium entitled, Undermining the Production of Race Science. We had a fantastic lineup of speakers, covering fields from quantitative genetics, to genomics, to the ecology of inter-group interactions, to the evolutionary psychology of judgement of risk and responsibility, to the basic (but fundamental) question of what do we mean when we use the word “group”? In addition to Charles and me, speakers included Robin Nelson (ASU), Adam Van Arsdale (Wellseley), Sheela Arthreya (TAMU), Clark Barrett (UCLA), Anne Pisor (WSU), Chelsea Cataldo-Ramirez (UC Davis), Ed Hagen (WSU), Kevin Bird (UC Davis), Fernando Villanea (Colorado), and Rebecca Sear (LSTHM). We also had two great discussants, Tina Lasisi (Michigan) and Rick Bribiescas (Yale), who helped us all process all the information.\nIt’s hard to overstate what a fantastic group of speakers we had. Everyone did their homework and showed up with—and delivered—an outstanding presentation. I learned a lot.\nAABA has an outstanding statement on race and racism that really lays out the broad outlines of the problems with race as an analytical category. Charles and I took a somewhat more technical tack on things. Our respective specialties are fairly formal (quantitative genetics and the evolution of life histories) and we see great value in formalizing evolutionary hypotheses in general, with the obvious application here to so-called “race science.” For example, if the hypothesis is that selection on intelligence has worked at the group level, you need to be able to show that there is a well-defined (i.e., not tautological) between-group variance on which, you know, selection can work! That is, after all, the way selection works. In his talk, which had the amazing title of “Do the Math or Someone Else Will Do It To You,” Charles noted that there isn’t.\nOr maybe you posit that there are continental/racial differences in timing of first birth, total fertility, parental investment, etc. as a result of trade-offs arising from energetic budget constraints. What is your fitness measure/have you specified an objective function? Obviously, you haven’t actually measured fitness (that would be asking an awful lot). Have you accounted for the actual energetic budget that produces the hypothesized trade-offs? Are you confusing trade-offs with constraints? Are the specified trade-offs even measurable?\nMany of the talks in our session had this same abductive quality to them: if some racist hypothesis were true, what would it entail from the actual science regarding genome banks, or admixture, or polygenic risk scores, gene regulatory networks, inter-group interactions, etc. Turns out, more often than not, it would entail impossible states of the world or at least states that are inconsistent with other core ideas in science.\nAnother theme that we emphasized was that we may fall back on idea that our science is being “misused,” but we might inadvertently be abetting race science through some of our less-examined conventions.\nOur symposium received a nice write-up in Science last week.\nI’ll end with a plot that I showed from one of the classics in the theory of life histories (van Noordwijk and De Jong 1986). They famously showed that life-history traits that theoretically trade off (and therefore, should be negatively correlated) can actually be positively correlated, depending on the magnitude of variance in available energy vs. allocation. A theory that depends on the existence of an unmeasurable trade-off is not a very good scientific theory.\n\n\n\nMy recreation of a plot by van Noordwijk and De Jong (1986) showing that if the variance in energy allocation across possible allocation phenotypes is less than the variance in energy availability across the environment, survival and reproduction, traits that are supposed to trade off, will actually show positive covariance."
  },
  {
    "objectID": "posts/aaba2024/index.html#linking-to-tescreal",
    "href": "posts/aaba2024/index.html#linking-to-tescreal",
    "title": "AABA 2024",
    "section": "Linking to TESCREAL",
    "text": "Linking to TESCREAL\nMy talk focused on some technical elements in formal theories of life-history evolution, some of which I discussed in a 2021 commentary in EHB with Daniel Promislow. However, I closed by linking race science to a broader intellectual movement dubbed TESCREAL by Timnit Gebru and Émile Torres. The acronym stands for Transhumanism, Extropianism, Singularitarianism, Cosmism, Rationalism, Effective Altruism, Longtermism. This is clearly material for another post, so I will just point the interested reader to a nice explainer that Émile wrote last year. Various elements of the TESCREAL movement have been linked to scientific racism and general white supremacy. In principle, the people in these movements care about science and rationality (the “R” in the acronym does stand for Rationalism, after all). As specialists in the various tools and theories that get appropriated by race scientists, we need to put in the effort to show how weak race science really is.\nIn his General Theory, John Maynard Keynes (1936: 96) wrote that “Practical men, who believe themselves to be quite exempt from any intellectual influences, are usually the slaves of some defunct economist. Madmen in authority, who hear voices in the air, are distilling their frenzy from some academic scribbler of a few years back.” I think that we can amend this to include self-styled free thinkers, whose brave speech somehow always manages to converge on old-fashioned prejudice, disparaging marginal groups and justifying their marginalization, are really just slaves to the hateful ideas of Lothrop Stoddard, Carleton Coon, and their hateful peers repackaged for the AI era."
  },
  {
    "objectID": "posts/termination_shock/index.html",
    "href": "posts/termination_shock/index.html",
    "title": "Termination Shock",
    "section": "",
    "text": "I just re-read Neal Stephenson’s Climate-Fiction novel, Termination Shock. I read this the moment it was published in 2021 and enjoyed it immensely. Having recently finished my winter class, Imagining Adaptive Societies, where I riffed on a few Stephenson-vs.-Robinson comparisons, and lacking anything I desperately wanted to read, it seemed like an opportune time to revisit.\nA few things jumped out to me on this second reading."
  },
  {
    "objectID": "posts/termination_shock/index.html#stephensons-vision-of-america-is-bleak",
    "href": "posts/termination_shock/index.html#stephensons-vision-of-america-is-bleak",
    "title": "Termination Shock",
    "section": "Stephenson’s Vision of America is Bleak",
    "text": "Stephenson’s Vision of America is Bleak\nThe novel is set in the near future. It’s not entirely clear when, but it isn’t too far away. For example, the main character, Frederika Mathilde Louisa Saskia, is the Queen of of the Netherlands and is forty-five years old in the book. Saskia, as she is known, is the granddaughter of Queen Beatrix, presumably the (fictional) daughter of King Willem-Alexander (described in both the book and Wikipedia as an “avid pilot”), who was born in 1967. As she is a fictional character, we can’t say for sure when she was born, but Willem-Alexander’s actual children were all born around the turn of the century. So, it could be as late as the 2040s, but I suspect a bit earlier. Saskia’s husband died following COVID infection while volunteering at a hospital. There is textual evidence for COVID pandemics in in 2023 and 2027, in addition to the 2019-2022 one we know all too well. This doesn’t tell us that much because it appears that COVID has become an annual event like influenza is currently (and COVID may already actually be). Saskia had the “latest and greatest” COVID vaccine before flying to Texas. This said, if the husband of a titular head of state is volunteering at a hospital, it is probably during a pandemic crisis, so let’s say 2027. This puts an upper date on when Saskia’s sixteen-year-old daughter, Lotte, could have been born. All told, this makes me think the novel takes place in the mid-2030s. The whole shit-show described by Termination Shock apparently takes place 15 or so years from now.\nIn a glowing review in Reason magazine, Peter Suderman suggests that Stephenson’s novel is a “Glorious Sci-Fi Vision of How To Respond to Global Warming,” one that favors innovation over legislation. I agree that there is lots of adaptation grist for the mill in Termination Shock, but the thought that the solutions people find in the book to cope with the increasing downsides of climate change are sufficient, or even preferable to actually addressing the root causes of the climate disaster is utter madness. Reading the chaos that Stephenson has written into the United States of the 2030s suggests to me that he can’t think it’s sufficient. He just (like me) is fascinated by people’s ingenuity and sees dramatic potential in these adaptations for, you know, a novel. Stephenson may have a reputation for anticipating new technologies, but ultimately, he is a fiction writer who has to tell a good story. This doesn’t mean that he actually thinks that the seat-of-pants adaptation depicted in the novel is “the very best we can do.”\nAs products of the Pleistocene, humans were born of climate change. Adapting is something we excel at and have always done. However, one of the most important—and impressive—adaptations our species possesses is the capacity to organize to achieve outcomes that are much greater than the sum of the individual inputs. Anthropogenic climate change is the greatest challenge for collective-action that Homo sapiens has ever faced. We simply must be up to the task.\nWhile the climate crisis is obviously global, in both reality and the novel, I want to focus on the situation in the United States that Stephenson depicts in Termination Shock. There are four features, in particular, that I read as striking warnings that this is not a world we want to let happen: the physical environment of much of the country has become essentially unlivable, endemic vector-borne infectious disease and the threat of pandemics are facts of life, there are terrifying and highly-destructive invasive species wreaking chaos, and the USA is apparently a failed—or at least failing—state. I seriously don’t think this is a near-future we want.\n\nUn-Survivable Wet-Bulb Temperatures\nTo survive the summer daytime heat in Texas and other parts of the American South, you need to wear a personal air conditioner. These are known as earthsuits.\nIn a warmer world, it will be increasingly difficult for people (and other organisms) to maintain physiological homeostasis—i.e., stay cool. Normal core temperature for humans is 37°C. Homeostatic mechanisms work to maintain this temperature and tolerances around it are actually quite tight: a 4° increase or 2° decrease can be life-threatening.\nHumans have at most about 30% mechanical efficiency. This means that 70% of the work we do ends up generating heat. Our primary mechanism for dumping excess heat in order to maintain homeostasis is the evaporative cooling that comes from sweating. For a thermal challenge to be compensable, there must be a water-vapor pressure gradient between the skin surface and the surrounding environment. If the air is saturated, there’s no gradient and the thermal challenge is uncompensable.\nWet-bulb temperature (WBT) is the lowest temperature that can be reached under ambient conditions by the evaporation of water only. As the WBT increases past 26°C (79°F), it starts to become dangerous. The 2003 European heat wave, which killed 20,000 people, maxed out with a WBT of 28°C. Anything over 30 is very dangerous and a WBT of 35°C (about 95°F) is not survivable by humans for more than about 20 minutes. Until quite recently, it’s been thought that a WBT of 35°C was effectively impossible under the prevailing climatic conditions of Earth, but as of 2020, there here have been at least 14 verified recordings of WBT in excess of the 35°C threshold of human survivability in the Persian Gulf and Pakistan since 1979. Climate change will increases these events. Modeling studies by Im and colleagues (2017) and Kang and Eltahir (2018) project heatwaves that will cross this critical threshold repeatedly in the next 30 years in the hugely-populous areas of the Gangetic Plain in India and the North China Plain in China. These two regions sum to more than a billion people.\nDangerous WBTs will be increasingly common in the United States as well. The worst regions look like they will be in the Mississippi River basin, spilling over into East Texas. These are areas where high summer temperatures combine with extreme humidity and that’s the combination that makes for dangers WBTs.\nIn Termination Shock, we read about people requiring earthsuits to work during the day in Waco and along the Brazos River to Houston, as well as at protests on the campus of Texas A&M in College Station and the West Texas ranch belonging to T.R. Schmidt, the billionaire who wildcats a geoengineering solution to global warming.\nEarthsuits are described as “a whole basket of technologies, some of which belonged to the realm of materials science, such as fabric, or of mechanical engineering, such as pumps and fans.” But really the core of the suit was a portable refrigeration system, which is an Einstein–Szilard refrigerator. It’s a cool idea (as it were) and might actually be workable in the not-too-distant future, though whether we will have functional suits (as opposed to structures) cooled by Einstein–Szilard refrigerator is an open question.\nThe simple fact is that the large swaths of the United States, including the second-largest state by population and the region that has driven population growth in recent years, are largely uninhabitable without substantial technological innovation and (presumably expensive) adaptation in the mid-2030s world of Termination Shock, is troubling to say the least. This strikes me as an outcome that is worth organizing to prevent.\n\n\nPerpetual Multidemic\nWe had a “tripledemic” of influenza, RSV, and SARS-CoV-2 in the winter of 2022. This was nothing compared to the ongoing infection pressure of the near future depicted in Termination Shock.\nQueen Saskia received “all the shots recommended for travel to Texas.” These included dengue, Zika, COVID, and malaria. It’s probably worth noting that none of these are currently endemic to Texas (except COVID). The good news is that we apparently have effective vaccines for dengue, Zika, and malaria in this timeline! There is a moderately-effective vaccine for dengue for use in children who have already had one dengue infection. Of course, its maker is discontinuing its manufacturing because it’s hard to make money off of a vaccine for a disease of mostly poor people of the Global South. It just be that way. Maybe if dengue becomes truly endemic in Texas and related locales, the profit motive will return. There’s no vaccine for Zika yet, but there is some promise, and malaria is a whole can of worms (Plasmodia, actually).\nAgain, this is not a future we should look forward to, even if we develop safe and effective vaccines, pathogens evolve and our recent experience teaches us that safe and effective vaccine availability doesn’t necessarily translate into wide adoption.\n\n\nFeral Hogs!\nIn the America of Termination Shock, we are well beyond 30-50 Feral Hogs. Imported European boars have hybridized with escaped agricultural giants to create truly terrifying beasts, running hog-wild in Texas, Louisiana, and beyond. One does wonder how these giant hogs manage to shed heat in an environment that requires humans to don personal refrigerators, but they still sound pretty terrifying. (presumably, these big hogs would need to keep very close to water at all times in such heat)\n\n\n\nDistribution of feral swine in the US in 2023 (USDA)\n\n\nThe swine are not the only invasive species that creates mischief in Termination Shock. Invasive fire ants, apparently attracted to the ozone of residential air-conditioning units, get into the units and interfere with the electrical relays that control the units, making them non-functional. In the Summer of the Great Relay Shortage, this happened on a massive scale, overwhelming the capacity of the supply chain to provide replacement relays and thereby displacing hundreds of thousands of Texans in the potentially deadly heat and humidity. (this, by the way, is another real phenomenon)\nSounds like a lot of fun.\n\n\nOminous Geopolitical Suggestions\nPossibility the most disturbing elements of the Termination Shock world center on the status of the United States of the near future. There is nothing explicit, but there are hints. For example, we are told that there is a “general inability of Americans to travel outside of the Lower 48.” The United States, which once seemed like an “omnipotent hyperpower,” is more like a “beached whale” in the Termination Shock world.\nDiscussing the possible consequences of T.R. Schmidt’s geoengineering experiment, the Scottish risk analyst, Alastair, says that since Texas would likely see Schmidt as a hero, any legal consequences would come down to “what’s left of the United States government.” He goes on to say that dysfunction in Congress and the courts virtually eliminates any legal risk whatsoever.\nWhen Canadian-Sikh stick fighter, Laks, is preparing his clandestine entry to the US, his Uncle Dharmender says, “the United States is a mess, sure. But it’s not like going to North Korea.” Well, at least we have that going for us.\nThe United States, apparently is “a basket case and global laughingstock.” It is “completely insane and out-of-hand,” a “clown show.” People rely on the “the sheer incompetence of the United States” in doing their business.\nDoes the libertarian ideal really involve giving up on the last best hope of Earth?"
  },
  {
    "objectID": "posts/termination_shock/index.html#is-this-really-what-we-want",
    "href": "posts/termination_shock/index.html#is-this-really-what-we-want",
    "title": "Termination Shock",
    "section": "Is This Really What We Want?",
    "text": "Is This Really What We Want?\nSuderman suggests that the crisis of climate change simply can’t be solved—it’s “too big, too complex, involving too many people, too many governments, too many interests. You can’t wrap your head around it. No one can.” Do we really believe that the sort of coping adaptations chronicled in Termination Shock are really “the very best we can do”?\nThe obvious contrast—and one that Suderman, in fact, makes—is the transformative, all-encompassing approach depicted in Kim Stanley Robinson’s Ministry for the Future.\nThe idea that we can either pursue a top-down governance where we attempt to change laws and norms or we pursue a piecemeal, bottom-up approach to adaptation is a false dichotomy. I recently saw a talk by and have since read the work of Charles Sabel and David Victor (book here; excerpt here). Sabel and Victor argue forcefully that this (and a couple other) false dichotomies are holding us back in our capacity to address the climate crisis, and suggest that we need to pursue an experimentalist approach to fixing climate change. The climate crisis is, indeed, a very difficult problem and we are awash in uncertainty over what the best ways to fix it even are. They write, “the only way to move beyond the status quo is to destabilize it, and then learn, quickly, to use the daring and imagination that bubble up in the open space to develop better approaches” (Sable & Victor 2022: 9). This is the bottom up. It is held together by what starts out as “a thin consensus” of individuals and institutions motivated to act. This consensus may be thin to start, but it needs to set bold goals. It’s these bold goals that hold the structure together, not the specific details of the plan. The thin consensus then “thickens with effort” as we learn about what—and who—works. This is top down. We need both if we are going to get through this crisis.\nTermination Shock is a remarkable book, with lots of interesting ideas. It is not a blueprint for how to solve the climate crisis, and I would be shocked if Neal Stephenson actually believed that it was. The bleakness of the portrait Stephenson paints for the United States of the near future makes it clear that this is something worth fighting against."
  },
  {
    "objectID": "posts/meng_paradox/index.html",
    "href": "posts/meng_paradox/index.html",
    "title": "Meng’s Big Data Paradox and the Recent Twitter Poll",
    "section": "",
    "text": "I decided to do a cocktail-napkin calculation this morning, as one does. Since Elon Musk bought the social-media platform formerly known as Twitter in October of 2022, there has been a clear shift to the right that many commentators have noted. Earlier this week, on the second day of the Democratic National Convention, Musk posted a Twitter poll about users’ preferences between the two presidential candidates, Harris and Trump. His poll, which collected nearly 5.85 million votes, revealed a 73% to 27% preference for Trump. Musk noted that the poll was “super unscientific,” but the vibe of the whole thing was that he was presenting the truth that the mainstream media is afraid to print. This has led some to worry that this is part of the ground game for claiming a rigged election if Trump loses.\nMost polls indicate that the contest is, in fact, very tight, with the two candidates in a nearly even race, though the most recent polls suggest a small advantage for Harris. This implies that Musk’s poll differs from the overall consensus by around 20-25 percentage points.\nAs with many others, I wondered if maybe this poll is telling us more about the composition of Twitter than it is about the US presidential election. There is good evidence that the platform has shifted substantially to the right. Musk’s own posts have also shifted considerably toward right-wing topics.\nIn 2018, the Harvard statistician Xiao-Li Meng wrote a paper where he coined the term The Big Data Paradox, where “the more the data, the surer we fool ourselves.” Appropriately enough for our current problem, Meng was trying to understand why the polls seemed to be so wrong in the 2016 US presidential election. Most polls had Trump losing and, of course, that didn’t actually happen.\nThat was about the time that our collective infatuation with big data was also taking off. The thinking that you don’t have to look hard to find is that with enough data, we don’t have to worry about tedious things like sampling design or measurement error. Generally there is some vague gesturing toward the Law of Large Numbers or the like.\nSuppose we have a sample of size \\(n\\) of a population of size \\(N\\) and we want to estimate some quantity (such as the share of a population’s vote) \\(G_N\\) from a our sample estimate \\(\\bar{G}_n\\).\nMeng derived an identity that shows that there are three (and only three) factors that determine the estimation error for some quantity of interest. These are:\n\nData quantity\nProblem difficulty\nData quality\n\nData quantity represents both the size of your statistical sample (\\(n\\)), but also the size of the population for which you are making your inference (\\(N\\)). You can think of those together as the fraction of the population sampled, \\(f=n/N\\).\nThe difficulty of the inference problem is given by the standard deviation of your quantity of interest (\\(\\sigma_G\\)). Meng notes that if \\(\\sigma_G=0\\), your inference problem is easy since you only need a sample size of \\(n=1\\) to infer it perfectly. As the standard deviation gets bigger, you need an increasingly large sample to make a good estimate.\nThe last quantity, data quality, is probably the most interesting. It is the correlation between the outcome (\\(G\\)) and the sampling mechanism (\\(R\\)). We can call this \\(\\rho_{G,R}\\).\nConsider the sample mean, \\(\\bar{G}_n\\). It’s a standard estimator of the quantity we’re interested in (i.e., \\(G_N\\)) because, as Meng notes, it’s consistent under both both design and model-based samples. We can write the sample mean in terms of what Meng calls its \\(R\\)-Mechanism, where \\(R\\) stands for Reporting, Responding, or Recording for big data or Random for a probabilistic sample. Specifically including the \\(R\\)-mechanism, the sample mean is:\n\\[\n\\bar{G}_n=\\frac{1}{n} \\sum_{j \\in I_n} Y_j=\\frac{\\sum_{j=1}^N R_j G_j}{\\sum_{j=1}^N R_j}.\n\\]\nHere \\(I_n\\) is a size-\\(n\\) subset of \\(\\{1, \\ldots , N\\}\\) (i.e., your sample) and \\(R_j=1\\) when \\(j\\) is in the set \\(I_n\\) and zero otherwise.\nWhen we gather a traditional design-based or model-based sample (for more on the distinction, check out Steve Thompson’s great book, Sampling), the \\(R_j\\) are random. However, for a convenience sample (like social media users responding to a poll), they are typically not. Meng (2018: 689) writes: “For many Big Data out there, however, they are either self-Reported or administratively Recorded, with no consideration for probabilistic sampling whatsoever. Even in cases where the data collector started with a probabilistic sampling design, as in many social science studies or governmental data projects, in the end we have only observations from those who choose to Respond, a process which again is outside of the probabilistic sampling framework. These ‘R-mechanisms’ therefore are crucial in determining the accuracy of \\(G_n\\) as an estimator of \\(G_N\\)”.\nWe can put this all together to show that the estimation error (i.e., the difference between our sample estimate and the population value) is given by the following remarkably simple formula:\n\\[\n\\hat{G}_n - \\hat{G}_N = \\rho_{G,R}\\;\\; \\times\\;\\; \\sqrt{\\frac{N-n}{n}}\\;\\; \\times\\;\\; \\sigma_G\n\\]\nIf \\(\\rho_{G,R}\\) is zero because your response indicator is random, then you have an unbiased estimate. That is, systematic error is zero and we are back in the familiar realm of probabilistic error with standard errors and the like.\nWe can apply this decomposition to Elon Musk’s poll.\n\n\n\n\n\nI used the following values: 154.6 million people voted in the presidential election in 2020. Turnout has been trending up, so we can use an estimate of \\(N=160,000,000\\) (using a bigger value for \\(N\\) makes the estimate more conservative). From the screen-captured image from Twitter, we can see that around 5.85M voted in the poll. As I noted above, \\(d = G_n - G_N = 0.23\\) (ish). I’m less certain about what a sensible value for the standard deviation of \\(G_N\\) is, but polls typically have a 5% “margin of error,” which seemed like as good a place to start as any. So now all we have to do is plug in and solve for \\(\\rho_{G,R}\\). The correlation I get is\n\n0.23/(sqrt((160-5.85)/5.85)*0.05) \n\n[1] 0.8961155\n\n\nThat seems like an absurdly high correlation. I was convinced that it had to be wrong, but if the input numbers are even approximately right, that correlation has to be very high to get such a big difference between the polling consensus of a very close race and the result of Musk’s Twitter poll with its value of approximately +50 for Trump. Now, of course, this assumes that the polls are a better reflection of reality and it was the failure of polls in 2016 that led to Meng’s paper in the first place. I am neither political scientist nor survey researcher, so I won’t comment on the quality of the current polls. But seriously, does anyone really believe that Trump has a large lead over Harris in our current social and political climate? I certainly hope that we’ve learned from our previous experience that big data do not guarantee correct inference and it certainly doesn’t obviate the need for proper sampling!\nWe can try various values of \\(\\sigma_G\\) as in the figure below. The high values of the standard deviations seem unlikely, given that the largest popular-vote margin since 2000 was 7.4% (Obama in 2012). Of course, it’s entirely possible that \\(G_N\\) isn’t 0.5 either. Reducing the error from the value of \\(d=0.23\\) to something lower (suggesting that Trump actually has a substantial lead among future voters) reduces the correlation in a linear fashion. However, in all of these cases, the value of \\(\\rho_{G,R}\\) remains relatively high compared to estimates of Meng (2018) or, e.g., the estimate of COVID vaccine uptake from Facebook surveys as reported by Bradley et al. (2021).\n\ncalc_meng &lt;- function(n=5.85e6,N=160e6,d=0.23,sigma) d/(sqrt((N-n)/n)*sigma)\nsig &lt;- seq(0.05,0.2,,100)\nplot(sig, calc_meng(sigma=sig), type=\"l\", lwd=2, col=\"blue4\", \n     xlab=expression(sigma[G]), \n     ylab=expression(rho[GR]))\n\n\n\n\n\n\n\n\nBut let’s be clear here: the value of the correlation is so high because the difference between Musk’s poll and the general polling consensus is so huge. Note also that this difference is even greater if Harris is actually leading as suggested by recent national polls.\nMeng (2018) found that a minuscule correlation of \\(\\rho_{G,R}=-0.005\\) for self-reported voting for Trump in 2016 in the Cooperative Congressional Election Study was enough to mess up the polls and catch everyone with their pants down. Bradley et al. (2021) estimated correlations between approximately 0.003 and 0.009 for the Delphi-Facebook COVID symptom tracker in the winter and spring of 2021, which overestimated actual vaccine uptake by approximately 14-20 percentage points. What does this much higher estimate (in absolute value) from Twitter mean for its composition in general?\nHonestly, I don’t know. It certainly suggests that people who follow Musk (and therefore were more likely to see the poll) are strongly biased to the right. Does that mean that Twitter as a whole is? Maybe. The next question, of course, is how representative Musk’s 193M followers are of the platform as a whole. But the algorithmic amplification of the site’s most-followed user means that even people who don’t follow Musk probably saw the post. I guess the bare minimum take-away here is to be very skeptical of Twitter polls that are meant to shed light on features of the general population.\nIt does seem clear that the platform formerly known as Twitter has become a hotbed of misinformation. Here is a helpful timeline of the changes in Twitter and Musk’s posting habits. It’s sad to watch. There was a time there when Scientific Twitter was an amazing place for communicating results and connecting to other scientists and science writers. I miss it."
  }
]